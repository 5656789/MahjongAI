{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Logistic Regression to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christoph\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "import sklearn.svm as svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29510, 13)\n",
      "Discard_0       29510\n",
      "Discard_1       29510\n",
      "Discard_2       29510\n",
      "Discard_3       29510\n",
      "Discard_4       29510\n",
      "Discard_5       29510\n",
      "Discard_6       29510\n",
      "Discard_7       29510\n",
      "Discard_8       29510\n",
      "hand            29510\n",
      "random_tile     29510\n",
      "waiting_tile    29510\n",
      "result          29510\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df['result'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29510, 10) (29510,)\n",
      "   Discard_0  Discard_1  Discard_2  Discard_3  Discard_4  Discard_5  \\\n",
      "0        8.0        9.0        7.0       33.0       18.0        6.0   \n",
      "2       30.0       33.0       18.0       19.0        2.0       32.0   \n",
      "4       23.0        1.0        3.0       32.0       19.0       28.0   \n",
      "6       19.0       10.0       33.0       21.0       32.0       26.0   \n",
      "8       29.0        1.0        2.0        6.0       21.0       25.0   \n",
      "\n",
      "   Discard_6  Discard_7  Discard_8  random_tile  \n",
      "0       29.0       16.0       20.0         26.0  \n",
      "2        9.0        9.0       27.0         14.0  \n",
      "4       11.0       33.0        1.0         19.0  \n",
      "6        0.0        6.0       31.0         19.0  \n",
      "8       11.0       33.0       13.0         16.0  \n",
      "0    0.0\n",
      "2    0.0\n",
      "4    0.0\n",
      "6    0.0\n",
      "8    0.0\n",
      "Name: result, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df[['Discard_0', 'Discard_1', 'Discard_2', 'Discard_3', 'Discard_4', 'Discard_5', 'Discard_6',\n",
    "       'Discard_7', 'Discard_8', 'random_tile']]\n",
    "y = df['result']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "print(X.shape, y.shape)\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5705853418593212 0.003 0.01\n",
      "0.6389572060993606 0.01 0.01\n",
      "0.6576487948844073 0.1 0.01\n",
      "0.6576487948844073 1 0.01\n",
      "0.6576487948844073 10 0.01\n",
      "0.6576487948844073 100 0.01\n",
      "0.6576487948844073 1000 0.01\n",
      "0.573536645351697 0.003 0.1\n",
      "0.6409247417609444 0.01 0.1\n",
      "0.6576487948844073 0.1 0.1\n",
      "0.6576487948844073 1 0.1\n",
      "0.6576487948844073 10 0.1\n",
      "0.6576487948844073 100 0.1\n",
      "0.6576487948844073 1000 0.1\n",
      "0.5784554845056566 0.003 0.2\n",
      "0.6404328578455485 0.01 0.2\n",
      "0.6576487948844073 0.1 0.2\n",
      "0.6576487948844073 1 0.2\n",
      "0.6576487948844073 10 0.2\n",
      "0.6576487948844073 100 0.2\n",
      "0.6576487948844073 1000 0.2\n",
      "0.6010821446138711 0.003 0.5\n",
      "0.6419085095917364 0.01 0.5\n",
      "0.6576487948844073 0.1 0.5\n",
      "0.6576487948844073 1 0.5\n",
      "0.6576487948844073 10 0.5\n",
      "0.6576487948844073 100 0.5\n",
      "0.6576487948844073 1000 0.5\n",
      "0.6192818494835219 0.003 0.6\n",
      "0.6433841613379242 0.01 0.6\n",
      "0.6576487948844073 0.1 0.6\n",
      "0.6576487948844073 1 0.6\n",
      "0.6576487948844073 10 0.6\n",
      "0.6576487948844073 100 0.6\n",
      "0.6576487948844073 1000 0.6\n",
      "Best Score:  0.6576487948844073\n",
      "gamma and nu:  0.1 0.1\n"
     ]
    }
   ],
   "source": [
    "nus = [0.01, 0.1, 0.2, 0.5, 0.6]\n",
    "gs = [0.003, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "best_score, best_g, best_nu = 0,0,0\n",
    "for nu in nus:\n",
    "    for g in gs:\n",
    "        model= svm.NuSVC(nu=nu, gamma=g)\n",
    "        model.fit(X_train, y_train)\n",
    "        temp_score = model.score(X_test, y_test)\n",
    "        print(temp_score, g, nu)\n",
    "        if(temp_score > best_score):\n",
    "            best_score, best_g, best_nu = temp_score, g, c\n",
    "print(\"Best Score: \", best_score)\n",
    "print(\"gamma and nu: \", best_g, best_nu)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score, g, c\n",
      "0.9605783350276742 0.003 0.1\n",
      "true acc:  0.0\n",
      "false acc:  1.0\n",
      "0.9605783350276742 0.01 0.1\n",
      "true acc:  0.0\n",
      "false acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "cs = [0.1, 1, 5, 10, 100]\n",
    "gs = [0.003, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "best_score, best_g, best_c = 0,0,0\n",
    "print(\"score, g, c\")\n",
    "for c in cs:\n",
    "    for g in gs:\n",
    "        model = svm.SVC(C=c, gamma=g, kernel='rbf')\n",
    "        model.fit(X_train, y_train)\n",
    "        temp_score = model.score(X_test, y_test)\n",
    "        predicted = model.predict(X_test)\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_test, predicted).ravel()\n",
    "        print(temp_score, g, c)\n",
    "        print(\"true acc: \", (tp/(fp+tn)))\n",
    "        print(\"false acc: \", (tn/(tn+fp)))\n",
    "        if(temp_score > best_score):\n",
    "            best_score, best_g, best_c = temp_score, g, c\n",
    "print(\"Best Score: \", best_score)\n",
    "print(\"gamma and c: \", best_g, best_c)\n",
    "        \n",
    "# svcmodel = svm.SVC(C=, gamma=0.003, kernel='rbf')\n",
    "# svcmodel.fit(X_train, y_train)\n",
    "# logmodel = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "# logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn fp fn tp\n",
      "947 390 489 207\n",
      "true acc:  0.15482423335826478\n",
      "false acc:  0.7083021690351533\n",
      "0.5676340383669454\n",
      "Predicted:  {0: 1436, 1: 597}\n",
      "Data:  0    1337\n",
      "1     696\n",
      "Name: result, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, predicted).ravel()\n",
    "print(\"tn fp fn tp\")\n",
    "print(tn, fp, fn, tp)\n",
    "print(\"true acc: \", (tp/(fp+tn)))\n",
    "print(\"false acc: \", (tn/(tn+fp)))\n",
    "print(model.score(X_test, y_test))\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)\n",
    "print(\"Predicted: \", dict(zip(unique_p, counts_p)))\n",
    "print(\"Data: \", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display accuracy compare with testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn fp fn tp\n",
      "1262 75 613 83\n",
      "true acc:  0.06207928197456993\n",
      "false acc:  0.943904263275991\n",
      "0.661583866207575\n",
      "Predicted:  {0: 1875, 1: 158}\n",
      "Data:  0    1337\n",
      "1     696\n",
      "Name: result, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# predicted = logmodel.predict(X_test)\n",
    "model = svm.SVC(C=1.0, gamma=0.003, kernel='rbf').fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, predicted).ravel()\n",
    "print(\"tn fp fn tp\")\n",
    "print(tn, fp, fn, tp)\n",
    "print(\"true acc: \", (tp/(fp+tn)))\n",
    "print(\"false acc: \", (tn/(tn+fp)))\n",
    "print(model.score(X_test, y_test))\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)\n",
    "print(\"Predicted: \", dict(zip(unique_p, counts_p)))\n",
    "print(\"Data: \", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuaacy of Cross-Validation: 0.6534830950760155\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(SVC(C=1.0, cache_size=600), X, y, scoring='accuracy', cv=10)\n",
    "print(\"Mean Accuaacy of Cross-Validation:\" ,scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict a new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65623778 0.34376222]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([0, 8, 9, 7, 33, 18, 6, 29, 16, 20])\n",
    "columns=['Discard_0', 'Discard_1', 'Discard_2', 'Discard_3', 'Discard_4', 'Discard_5',\n",
    "         'Discard_6', 'Discard_7', 'Discard_8', 'random_tile']\n",
    "df2 = pd.DataFrame(data.reshape(-1, len(data)),columns=columns)\n",
    "print(logmodel.predict_proba(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaacy of KNN: 0.6542056074766355\n",
      "Predited 1: 143\n",
      "Predited 0: 1890\n",
      "Actual 1: 696\n",
      "Actual 0: 1337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "print(\"Accuaacy of KNN:\", metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "a = np.count_nonzero(predicted == 1)\n",
    "print(\"Predited 1:\",a)\n",
    "b = np.count_nonzero(predicted == 0) \n",
    "print(\"Predited 0:\", b)\n",
    "c = np.count_nonzero(y_test == 1)\n",
    "print(\"Actual 1:\", c)\n",
    "d = np.count_nonzero(y_test == 0) \n",
    "print(\"Actual 0:\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual True: 696\n",
      "Predicted True: 68\n",
      "Accuracy: 0.09770114942528736\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted).ravel()\n",
    "print(\"Actual True:\", tp+fn);\n",
    "print(\"Predicted True:\", tp);\n",
    "print(\"Accuracy:\", (tp/ (tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4428\n",
       "1    2348\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['result'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaacy of Neural Network: 0.5868175110673881\n",
      "Predited 1: 662\n",
      "Predited 0: 1371\n",
      "Actual 1: 696\n",
      "Actual 0: 1337\n",
      "Actual True: 696\n",
      "Predicted True: 259\n",
      "Accuracy: 0.37212643678160917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(110,110,110),max_iter=60000)\n",
    "mlp.fit(X_train,y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "print(\"Accuaacy of Neural Network:\", metrics.accuracy_score(y_test, predictions))\n",
    "a = np.count_nonzero(predictions == 1)\n",
    "print(\"Predited 1:\",a)\n",
    "b = np.count_nonzero(predictions == 0) \n",
    "print(\"Predited 0:\", b)\n",
    "c = np.count_nonzero(y_test == 1)\n",
    "print(\"Actual 1:\", c)\n",
    "d = np.count_nonzero(y_test == 0) \n",
    "print(\"Actual 0:\", d)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(\"Actual True:\", tp+fn);\n",
    "print(\"Predicted True:\", tp);\n",
    "print(\"Accuracy:\", (tp / (tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
